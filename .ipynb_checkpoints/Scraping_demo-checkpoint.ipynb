{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Web Scraping Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we scrape all the info needed from a page of yelp for our improved search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from lxml.html import fromstring\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import traceback\n",
    "from urllib.parse import urljoin,urlparse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame()\n",
    "user_df = pd.DataFrame()\n",
    "resturant_df = pd.DataFrame()\n",
    "\n",
    "resturants_columns = ['resturant_id','resturant','resturant_rating','cuisine','resturant_review_count']\n",
    "reviews_columns = ['resturant_id','user_id','description','rating','date','useful','funny','cool']\n",
    "users_columns = ['user_id','name','user_review_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxies():\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "\n",
    "    webdriver_name = 'geckodriver'\n",
    "    driver = webdriver.Firefox(executable_path = 'webdriver/geckodriver',   options=options)\n",
    "\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    driver.get(url)\n",
    "\n",
    "    https_button = driver.find_element_by_xpath('/html/body/section[1]/div/div[2]/div/div[2]/div/table/thead/tr/th[7]')\n",
    "    https_button.click()\n",
    "    https_button.click()\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    possible_proxies = soup.find_all('tbody')[0].find_all('tr')\n",
    "    proxies = set()\n",
    "    for possible_proxy in possible_proxies:\n",
    "        possible_proxy_info = possible_proxy.find_all('td')\n",
    "        if possible_proxy_info[6].text == 'yes':\n",
    "            proxy = possible_proxy_info[0].text + \":\" + possible_proxy_info[1].text\n",
    "            proxies.add(proxy)\n",
    "\n",
    "    driver.close()\n",
    "    return(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection(url):\n",
    "    proxy = next(proxy_pool)\n",
    "    flag = 0\n",
    "    while not flag:\n",
    "        try:\n",
    "            r = requests.get(url,proxies={\"http\": proxy, \"https\": proxy})\n",
    "            flag = 1\n",
    "        except:\n",
    "            print(r.json())\n",
    "            print(\"Skipping. Connnection error\")\n",
    "\n",
    "    return(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_scrapper(review):\n",
    "    name = review.find('a',{'class':\"user-display-name\"}).text\n",
    "    user_id = review.find('a',{'class':\"js-analytics-click\"}).get('href').split('=')[1]    \n",
    "    review_count = int(review.find('li',{'class':\"review-count responsive-small-display-inline-block\"}).find('b').text)\n",
    "    \n",
    "    review_text = review.find(\"p\").text\n",
    "    rating = float(review.find('div',{'class':\"i-stars\"}).get('title')[0:3])\n",
    "    review_date = review.find('span',{'class':\"rating-qualifier\"}).text.strip().split(\"\\n\")[0]\n",
    "    \n",
    "    vote_buttons = review.find_all('span',{'class':\"count\"})[0:3]\n",
    "    votes = []\n",
    "    for button in vote_buttons:\n",
    "        if button.text == '':\n",
    "            votes.append(0)\n",
    "        else:\n",
    "            votes.append(int(button.text))\n",
    "    \n",
    "    user_lst = [user_id,name,review_count]\n",
    "\n",
    "    review_lst = [user_id,review_text,rating,review_date]\n",
    "    review_lst.extend(votes)\n",
    "    return(user_lst,review_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resturant_scrapper(url):\n",
    "    r =  requests.get(url)\n",
    "    soup = BeautifulSoup(r.content,'html.parser')\n",
    "    \n",
    "    resturant_id = url.split(\"biz/\")[1]\n",
    "\n",
    "    resturant_name_lst = soup.find_all('h1')\n",
    "    resturant = ''\n",
    "    for resturant_name in resturant_name_lst:\n",
    "        resturant += resturant_name.text + \" \"\n",
    "    resturant = resturant.strip()\n",
    "    \n",
    "    resturant_rating = float(soup.find('div',{'class':'i-stars'}).get('title')[0:3])\n",
    "\n",
    "    reviews_count = int(soup.find(\"span\",{\"class\":\"review-count rating-qualifier\"}).text.strip().split(' ')[0])\n",
    "    \n",
    "    cuisine_lst = soup.find(\"span\",{\"class\":\"category-str-list\"}).find_all('a')\n",
    "    cuisines = [cuisine.text for cuisine in cuisine_lst]\n",
    "    resturant_lst = [resturant_id,resturant,resturant_rating,cuisines,reviews_count]\n",
    "    resturants_df = pd.DataFrame([resturant_lst])\n",
    "\n",
    "    reviews_df = pd.DataFrame()\n",
    "    users_df = pd.DataFrame()\n",
    "    while True:\n",
    "        reviews = soup.find_all('div',{'class':'review review--with-sidebar'})\n",
    "        for review_input in reviews:\n",
    "            user_lst,review_lst = review_scrapper(review_input)\n",
    "\n",
    "            review_data = [resturant_id]\n",
    "            review_data.extend(review_lst)\n",
    "            \n",
    "            review_df = pd.DataFrame([review_data])\n",
    "            reviews_df = pd.concat([reviews_df,review_df],axis = 0)\n",
    "            \n",
    "            user_df = pd.DataFrame([user_lst])\n",
    "            users_df = pd.concat([users_df,user_df],axis = 0)\n",
    "            \n",
    "        next_page = soup.find('a',{'class':'u-decoration-none next pagination-links_anchor'})\n",
    "        if next_page is None:\n",
    "            break\n",
    "\n",
    "        url = next_page.get('href')\n",
    "        time.sleep(4 + random.random())\n",
    "        r =  requests.get(url)\n",
    "        soup = BeautifulSoup(r.content,'html.parser')\n",
    "\n",
    "    return(users_df,resturants_df,reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resturants(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content,'html.parser')\n",
    "    \n",
    "    resturants_to_search = []\n",
    "    while True:\n",
    "        resturant_tags = soup.find_all(\"a\",{\"class\":\"lemon--a__373c0__IEZFH link__373c0__29943 link-color--blue-dark__373c0__1mhJo link-size--inherit__373c0__2JXk5\"})\n",
    "        resturants_to_add = set([urlparse(resturant_tag.get('href')).path for resturant_tag in resturant_tags])        \n",
    "        resturants_to_search.extend(resturants_to_add)\n",
    "        \n",
    "        next_page = soup.find(\"a\",{\"class\":\"lemon--a__373c0__IEZFH link__373c0__29943 next-link navigation-button__373c0__1D3Ug link-color--blue-dark__373c0__1mhJo link-size--default__373c0__1skgq\"})\n",
    "        \n",
    "        if next_page is None:\n",
    "            break\n",
    "            \n",
    "        url = base_url + next_page.get('href')\n",
    "        \n",
    "        time.sleep(2+random.random())\n",
    "        r =  requests.get(url)\n",
    "        soup = BeautifulSoup(r.content,'html.parser')\n",
    "    print(\"[INFO] Complete\")\n",
    "    print(\"[INFO] Resturant Count: %i\" %len(resturants_to_search))\n",
    "    \n",
    "    return(resturants_to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Complete\n",
      "[INFO] Resturant Count: 389\n"
     ]
    }
   ],
   "source": [
    "#proxies = get_proxies()\n",
    "#proxy_pool = cycle(proxies)\n",
    "\n",
    "base_url = 'http://www.yelp.com'\n",
    "hoboken_yelp = base_url + \"/search?find_desc=&find_loc=Hoboken\"\n",
    "resturant_urls = get_resturants(hoboken_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/biz/la-isla-restaurant-hoboken\n",
      "/biz/mamouns-falafel-restaurant-hoboken\n",
      "/biz/the-cuban-restaurant-and-bar-hoboken-2\n",
      "/biz/fiore-deli-of-hoboken-hoboken\n",
      "/biz/bw%C3%A8-kafe-hoboken\n",
      "/biz/karma-kafe-hoboken\n",
      "/biz/m-and-p-biancamano-hoboken\n",
      "/biz/amandas-restaurant-hoboken-2\n",
      "/biz/vitos-italian-deli-hoboken\n",
      "/biz/anthony-davids-hoboken\n",
      "/biz/tutta-pesca-hoboken\n",
      "/biz/benny-tudinos-pizzeria-hoboken\n",
      "/biz/choc-o-pain-hoboken-2\n",
      "/biz/sweet-hoboken\n",
      "/biz/empire-coffee-and-tea-company-hoboken\n",
      "/biz/old-german-bakery-hoboken\n",
      "/biz/satay-malaysian-cuisine-hoboken-2\n",
      "/biz/elysian-cafe-hoboken\n",
      "/biz/grimaldis-hoboken\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-4ed1ac414522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresturant_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0musers_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresturants_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreviews_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresturant_scrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresturant_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musers_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mresturants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresturants\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresturants_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-79936f2867ce>\u001b[0m in \u001b[0;36mresturant_scrapper\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mresturant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresturant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresturant_rating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'i-stars'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mreviews_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"span\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"review-count rating-qualifier\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "users = pd.DataFrame()\n",
    "resturants = pd.DataFrame()\n",
    "reviews = pd.DataFrame()\n",
    "for resturant_url in resturant_urls:\n",
    "    print(resturant_url)\n",
    "    time.sleep(15+random.random())\n",
    "    users_df,resturants_df,reviews_df = resturant_scrapper(base_url + resturant_url)\n",
    "    users = pd.concat([users,users_df],axis = 0)\n",
    "    resturants = pd.concat([resturants,resturants_df],axis = 0)\n",
    "    reviews = pd.concat([reviews,reviews_df],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.columns = users_columns\n",
    "resturants.columns = resturants_columns\n",
    "reviews.columns = reviews_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.to_csv('data/users.csv')\n",
    "resturants.to_csv('data/resturants.csv')\n",
    "reviews.to_csv('data/reviews.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
